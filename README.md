Welcome to the ML Challenge/Contest @ Algorithms Summit 2024.
This repository represents a starter kit for participants to get started with the challenge.

---
#### Overview

"Have you ever experienced the frustration of fixing a bug during the design stage only to discover additional bugs while trying to fix the existing one? As a verification engineer, this can be a common problem that can cause previously passing tests to fail suddenly, leaving your team to pick up the pieces." [Cadence](https://community.cadence.com/cadence_blogs_8/b/fv/posts/automate-regression-failure-triage-with-the-cadence-verisium)

This ML challenge is concerned with automating regression failure triage: clustering failures with common root cause. For more details on failure triage, please see [this](https://community.cadence.com/cadence_blogs_8/b/fv/posts/automate-regression-failure-triage-with-the-cadence-verisium)

We would like to do this by analyzing log files generated by the running tests. Traditional approaches employ regular expressions and basic NLP techniques to identify similarities between log files. The rationale is that _similar_ log files have _similar_ root causes.


To this end, this challenge focuses on using recent advances in GenAI (e.g., text embedding models) to cluster a corpus of regression log files that our colleague Kaushal Modi has provided.

Note that this year's challenge has more of an `unsupervised learning` framework than that of a `supervised` one.

In other words, we do not have a concrete idea of what cluster label each log file should take.

All we have is a bunch of log files with some prior knowledge on how many clusters they belong to (around 4-8 clusters)
and a labelling of some of the log files (provided in `data/log_metadata/mlchallenge.csv`) that we are providing here
to guide the participants as they build their clustering pipeline.

Think of the labels in `data/log_metadata/mlchallenge.csv` as noisy labels - considered good but still not perfect.

---
#### Get Started

Please go through the following documents to get started with the challenge.

1. [Setting Python Environment/Dependencies](https://gitlab.analog.com/aaldujai/ml_challenge_as24/-/blob/master/docs/PYENV.md)
2. [Running Notebooks and Python Scripts](https://gitlab.analog.com/aaldujai/ml_challenge_as24/-/blob/master/docs/RUN.md)
2. [Challenge Dataset and Submission Format](https://gitlab.analog.com/aaldujai/ml_challenge_as24/-/blob/master/docs/DATA.md)
3. [Challenge Submission and Evaluation](https://gitlab.analog.com/aaldujai/ml_challenge_as24/-/blob/master/docs/EVAL.md)

The slides for an overview of the challenge can be found [here](https://gitlab.analog.com/aaldujai/ml_challenge_as24/-/blob/master/docs/Challenge-Overview.pptx).

---
### Q&As Dataset Assumptions/Limitations

We will populate this section with frequently asked questions about this challenge.


---
### Understanding Embeddings

The provided embeddings are produced by passing the log files to a neural network called an embedding model. This model takes as input a block of text, and returns a vector which represents the contents of that text. The goal of the embedding model is to produce similar vectors for similar text, and dissimilar vectors for dissimilar text.

There is a limit to how much text the embedding model can process at once, and thus we cannot simply feed in an entire log file and get back a single vector. Instead, we break the log file up into chunks, and produce a vector for each chunk. Stacking these vectors produces a matrix which represents the entire log file and in which each row represents an individual chunk. In particular, we divide the log file into chunks of 100 lines, with a 50 line overlap between adjacent chunks. This way, we avoid the possibility of a chunk boundary separating a line from its nearby context.



For more details on the embedding process, including the specific embedding model used and some helpful references, please refer to [this](https://gitlab.analog.com/aaldujai/ml_challenge_as24/-/blob/master/docs/DATA.md#embeddings) and the `scripts/embed_mlchallenge_data.py` Python script.


---
#### Ideas / Suggestions to beat the baselines

To familiarize yourself with the challenge, please take your time to go through the `notebooks/simple_baseline.ipynb` notebook.

The notebook discusses several aspects of the challenge and closes with some thoughts on possible approaches. Some of these thoughts are listed here.

- Do not take embeddings at face value. Train an ML model that improves embedding-based retrieval.
- Embedding logic: the current chunking strategy of the log files does not consider the semantic content and structure of the files. Is there a better alternative? One further question, is embedding even required? Can one inspect these log files and figure out a better alternative of featurizing them?
- Distance/Similarity between log files embeddings: In the above notebook, we made use of a correlation measure between a pair of matrices. Such measure can detect linear relationships but not non-linear ones. Is there a better alternative? Also, we are trying to compute similarities between two `sets` of embeddings that can be of variable size and order. Is there a better alternative?
- We did not touch on the scalability of the clustering pipeline. We assume everything fits in memory but that is something to think about.
- Clustering Interpretability: can the clustering pipeline we create explain the decisions it takes to cluster the log files (e.g., by pointing to relevant lines into the log files, etc)?

---
#### Feedback / Questions / Bug Reports

Feel free to share your experience on the [TEAMS group](https://teams.microsoft.com/l/team/19%3AsPkLqzyDM78MDQq5LbWL2doQgc9LjbGEMjlReS6MkT01@thread.tacv2/conversations?groupId=aa7e9374-9bed-4cf3-9c88-b387c82f30da&tenantId=eaa689b4-8f87-40e0-9c6f-7228de4d754a). Or simply raise an issue for this repository.

---
#### ADI Generative AI Policy

As we go through the nuts and bolts of this challenge, let's remind ourselves with ADI GenAI policy and understand what can and can not be done with GenAI for ADI. As long as the information that we share with GenAI tools is publicly available, there should not be an problem with ADI's policy. More details can be found [here](https://analog.sharepoint.com/sites/LRO/Shared%20Documents/Forms/AllItems.aspx?id=/sites/LRO/Shared%20Documents/policies%20%26%20procedures/Generative%20AI%20Policy%20-%20Final.pdf&parent=/sites/LRO/Shared%20Documents/policies%20%26%20procedures).


---
#### Acknowledgement

We would like to thank the following
1. Nimay Shah and  Kaushal Modi for introducing this problem, providing the relevant dataset, sharing feedback on the noisy labels, and contributing to the slides.
2. Nick Moran and Peter Cho for helpful discussion on chunking log files.
3. Tianyu Dai, Varun Kelkar, Zhiwu Zheng, and Nick Moran for constant feedback on several aspects of this challenge.



